<!DOCTYPE html>
<html>
<head>
    <title>os1</title>

    <link rel="stylesheet" href="style.css" />

    <!-- syntax highlighting -->
    <link href="prism.css" rel="stylesheet" />
    <script src="prism.js"></script>
    <style>
#ofs-table {
    width: 100%;
    background: #ececec;
}

#ofs-table, #ofs-table tr, #ofs-table td, #ofs-table th {
    border: 1px solid #ccc;
    border-collapse: collapse;
}

#ofs-table td, #ofs-table th {
    padding: 5px;
}

#ofs-table th {
    font-weight: bold;
    text-align: left;
    background: #e0e0e0;
}

#ofs-table td:first-child {
    text-align: center;
    font-family: monospace;
}
    </style>
</head>
<body>
<div id="nav-container-outer">
    <div id="nav-container">
        <a href="index.html">os1</a>
        <span class="nav-separator"></span>
        <a href="implementation.html">Implementation</a>
        <span class="nav-separator"></span>
        <a href="doc/os1/index.html">Rust Docs</a>
        <span class="nav-separator"></span>
        <a href="https://github.com/mark-i-m/os1">Source</a>
    </div>
</div>
<div id="body-container">

    <h1>Booting</h1>
    <p>
    The OS uses a simple two-stage bootloader in the MBR of the disk. It was
    not designed for compatibility; I haven't tested it on any other VM
    software, other than QEMU.
    </p>

    <p>
    The bootloader does a few things, in this order:

    <ol>
        <li>Disable interrupts</li>
        <li>
        Print an 'x' to the console (in QEMU, this is represented in the serial
        console, not the VGA display)
        </li>
        <li>
        Load the second-stage bootloader from the next segments of memory
        </li>
        <li>
        Transfer control to the second-stage bootloader, which carries out the
        rest of these steps:
        </li>
        <li>
        Find copy the kernel from the disk into memory starting
        <code>loadKernelHere</code>, which is defined in <code>mbr.S</code>
        </li>
        <li>
        Exectute the E820 BIOS call to get a map of available and reserved
        physical memory regions.
        </li>
        <li>Do a long jump to start the kernel.</li>
    </ol>
    </p>

    <p>
    All of this is defined in <code>mbr.S</code>. <code>mbr.S</code> also
    defines the GDT and IDT and the various segments. These variables are
    global and can be used elsewhere in the OS source.
    </p>

    <h1>Kernel startup</h1>

    <p>
    The <code>kernel_main</code> function in <code>lib.rs</code> is the Rust
    entry point of the kernel; it is the first Rust code to run. The function
    initialized kernel subsystems in the following order:

    <ol>
        <li>Initialize the TSS</li>
        <li>Initialize the kernel heap manager</li>
        <li>Initialize physical memory management</li>
        <li>Create the init and idle processes</li>
        <li>Initialize the PIC</li>
        <li>Initialize the PIT</li>
        <li>Load the root file system from HDD</li>
        <li>Jump to the init process</li>
    </ol>
    </p>

    <p>
    Starting the init process accomplishes a few things. First, it turns on
    interrupts for the first time. Second, it turns on virtual memory for the
    first time. Third, it is the first real process to run in the system,
    allowing it to spawn other processes.
    </p>

    <h1>Processes and Scheduling</h1>

    <p>
    Processes have a

    <ul>
        <li>Unique 32-bit PID</li>
        <li>Rust string representing the name (not necessarily unique, but it
        helps with debugging)</li>
        <li>2048-word kernel-mode stack allocated on the kernel heap (this stack
        is not used when the process is in user-mode)</li>
        <li>Context struct representing the context at which the process was
        interrupted (only valid if the process is NOT running)</li>
        <li>State (running, blocked, ready, terminated)</li>
        <li>AddressSpace struct with a pointer to that process's page
        directory</li>
        <li>Function pointer to the process's routine (specified at process
        creation)</li>
        <li>Buffer of characters typed while this process had focus</li>
        <li>Current working file (the os1 equivalent of a `pwd`)</li>
    </ul>
    </p>

    <p>
    Unfortunately, because processes are allocated on the kernel heap, Rust
    will not allow them to use closures instead of function pointers. It would
    be nice if we could have each process contain a dynamically bound call
    using the <code>FnOnce</code> trait (essentially a thunk), but
    <code>FnOnce</code>'s <code>call</code> method moves the
    <code>FnOnce</code> out of the process, which is not allowed because
    process is heap allocated and has a destructor. There is an <a
        href="https://github.com/rust-lang/rust/issues/28796">open RFC</a>
    about the <code>Box<FnBox></code> constuct, which would allow the desired
    behavior, but it is not working yet.  If this RFC was implemented, we would
    be able to create processes like so:
    </p>

    <pre><code class="language-rust">
    let resource = Arc::new(Semaphore::new(foo));
    let r = resource.clone();
    process::make_ready(Process::new("bar_proc", move |this| {
        println!("{:?} says {}", this, r.down());
    }));
    </code></pre>

    <p>
    Processes are round-robin scheduled, according to the order in which
    <code>process::make_ready</code> is called. A process can yield the rest of
    its quantum by calling <code>process::proc_yield</code>. os1 uses preemptive
    multiprocessing based on the PIT. This is largely invisible from the point
    of view of processes, though.
    </p>

    <h1>Memory</h1>

    <p>
    From the point of view of the kernel, memory is split up into 3 sections.
    The first megabyte of memory consists of the kernel code, the vga buffer,
    and various constants. The next 3MiB of memory contain the kernel heap. The
    following 4MiB of memory contain the physical frame data structures (more
    later). These first 8MiB of memory are direct mapped for convenience. The
    next 8MiB of the virtual address space is mapped to the page directory.
    Finally, the next MiB of the virtual address space is reserved for the
    kernel to map as needed. The rest of the virtual address space is available
    in user mode.
    </p>

    <h2>Heap</h2>

    <p>
    The kernel heap is used for kernel data structures only; when a process
    switches to user-mode, its heap and stack are mapped to physical memory
    frames via page tables.  The kernel heap is a first-fit heap implementation
    with block coalescing and splitting.  In my implementation, each free block
    has a 3-word header consisting of the size of the block and forward and
    backward pointers to other free blocks, along with some flags.  Each block
    has a footer than consists only of the size of the block.
    </p>

    <p>
    Because the Rust compiler is the only entity making calls to allocate or
    free memory, less sanity checking and metadata is necessary than normal.
    This allows some simplifying assumptions and gives the heap implementation
    some nice properties that are not often possible in C:

    <ul>
        <li>
        Used blocks have no metadata at all. The compiler makes calls to
        <code>free</code>, passing in the size of the block in addition to a
        pointer to the block. This means that we always know as much information
        as we need to add a block to the free list and find the next block to
        merge with it if it is free. The compiler guarantees that calls to free
        will only be for legitimate used blocks.
        </li>
        <li>All blocks are an exact multiple of <code>4*WORD_SIZE</code> in
        size.</li>
        <li>All blocks are <code>(4*WORD_SIZE)</code>-byte-aligned.</li>
    </ul>
    </p>

    <p>
    This allows for easy sanity checking and alignment guarantees, while there
    is also no overhead for metadata.
    </p>

    <h2>Physical memory</h2>

    <p>
    The kernel only reserves the first 8MiB of physical memory for its own use.
    Using the E820 memory map obtained at boot, the physical memory allocator
    initializes its data structures at startup. Each physical frame in upto 4GiB
    of memory has a 1-word descriptor (<code>struct FrameInfo</code>).  If a
    frame is free, this descriptor is used to keep the index of the next free
    frame, so that all free frames are described by the LIFO list formed by all
    of the descriptors.
    </p>

    <p>
    A frame is allocated by removing its <code>FrameInfo</code> from the free
    list, and conversely, a frame is freed by adding its <code>FrameInfo</code>
    to the list. When a frame is allocated, its <code>FrameInfo</code> is used
    to hold a pointer to a data structure describing all pages in all processes
    that are mapped to that frame.
    </p>

    <h2>Virtual Memory</h2>

    <p>
    The virtual address space of a process consists of all addresses for
    <code>0xD0_0000-0xFFFF_FFFF</code>.  When a page fault occurs (in kernel or
    user mode), if the faulting address is in this range, the fault is
    considered an error, and the process is killed. This is because the first
    8MiB are shared, direct-mapped memory and should never produce a page fault.
    The next 4MiB are mapped to the processes page directory, so if there is a
    fault in this range, it is fatal, since it means the paging structures have
    been corrupt. The 13MiB is claimed by the kernel for use by the
    <code>kmap</code> function, which temporarily maps a page meant to reside in
    another process's address space. This is needed for creating a new process.
    Luckly, with Rust, segfaults should be extremely rare.
    </p>

    <p>
    When a page fault occurs for a legal address, the fault handler checks the
    following cases:

    <ol>
        <li>Is the page paged out to disk? If so, swap it back in</li>
        <li>Is the page marked present but read-only? If so, this page is COW,
        so clone it and mark the new page read/write</li>
        <li>Else, allocate a new frame a map the page to it</li>
    </ol>
    </p>

    <h1>Concurrency Control</h1>

    os1 uses preemptive multiprocessing via interrupts. os1 has two options for
    concurrency control (so far). The first is to simply turn off interrupts.
    The second is to use a semaphore.

    <h2>Interrupts</h2>

    <p>
    Turning off interrupts is only an option in kernel mode. It is the option
    of last resort. In some cases, it is unavoidable, such as code for context
    switching. When I turn on SMP, a spin lock will also be required in places
    where turning off interrupts was used.
    </p>

    <h2>Semaphores</h2>

    <p>
    os1 has semaphore implementation based on Rust's <code>Mutex</code> type.
    The <code>Semaphore<T></code> type
    only has a <code>clone</code> method, signifying that the process would like
    to acquire the resource. This returns a RAII <code>SemaphoreGuard</code>.
    When the guard's lifetime ends, the resource is automatically released.
    </p>

    <p>
    Like Rust's native <code>Mutex</code> (which unfortunately cannot be
    used because it lives in <code>libstd</code> and relies on libc), the
    <code>Semaphore</code> takes ownership of the data it protects. Thus, Rust
    will prevent illegal or unsafe accesses to the data at compile time.
    </p>

    <p>
    Semaphores are available via a system call to user processes. I haven't
    decided how this will look yet in user mode. In kernel mode, it looks
    something like this:
    </p>

    <pre><code class="language-rust">
    // wrap resource in a semaphore with initial count 3
    let s = Semaphore::new(resource, 3);

    {
        // acquire
        let guard = s.down();
    }
    // guard dies => release
    </code></pre>

    <h2>Barriers and Events</h2>

    <p>
    os1 also has two other semaphore-based concurrency primitives. The first is
    <code>Barrier</code>. When the barrier is constructed, it is given a parameter
    <code>N</code>. The first <code>N-1</code> processes that reach the barrier
    will block. When the <code>N</code>th process reaches the barrier, all processes
    will unblock from the barrier.
    </p>

    <pre><code class="language-rust">
    let b = Barrier::new(3); // A barrier for 3 processes

    ...

    // In process A

    b.reach();
    // If this is the 3rd process to reach the barrier, the first two will unblock;
    // otherwise, this process will block until the third reaches the barrier.
    </code></pre>

    <p>
    The second primitive is <code>Event</code>. When a process waits on the event, it
    will block until someone &quot;notifies&quot; the event. An unlimited number of
    processes can wait on an event.
    </p>

    <pre><code class="language-rust">
    let e = Event::new(); // An event

    ...

    // In process A

    e.wait(); // Process A blocks

    ...

    // In process B
    e.notify(); // Wakes up Process A and any others waiting
    </code></pre>

    <h1>Inter-process Communication (IPC)</h1>

    <p>
    In os1, IPC is accomplished via shared pages. A process can request to share
    a page with another process. It will block until the other process accepts the
    request. Symmetrically, a process can request another process to share a page
    with it, and it will block until the request is fullfilled.
    </p>

    <pre><code class="language-rust">
    // In process A

    // This call will share with the process with PID = 3 the frame mapped to 0xC000_0000.
    // The process will block until this call returns.
    if !(*CURRENT_PROCESS).addr_space.request_share(/* pid */ 3, /* vaddr */ 0xC000_0000) {
        // If the call returns false, then an error has occured (e.g. process 3 died)
    }

    ...

    // In process B

    // This call will accept a shared page from the process with PID = 2. The accepted frame
    // will be mapped to 0xD000_0000 in this address space. Note that this address does not
    // have to be the same as the sharing process. This process process will block until the
    // call returns
    if !(*CURRENT_PROCESS).addr_space.accept_share(/* pid */ 2, /* vaddr */ 0xD000_0000) {
        // If the call returns false, then an error has occured (e.g. process 2 died)
    }
    </code></pre>

    <h1>Display driver</h1>

    <p>
    os1 has a primitive display driver for QEMU in VGA mode. It features some very basic
    abstractions to make drawing and printing strings easier. Including bounded boxes and
    word wrap.
    </p>

    <h1>I/O</h1>

    <h2>Keyboard</h2>

    <p>
    There is a simple keyboard-input mechanism. It mostly supports ASCII
    alpha-numeric characters.
    </p>

    <h2>Block Devices</h2>

    <p>
    There is also very simple IDE support via PIO (the old fashion way). This
    is used mostly to support the file system.
    </p>

    <h1>os1 File System (OFS)</h1>

    <p>
    Like IPC, this is one of my larger departures from the Unix world. OFS is a
    simple, graph-based file system. It does not have directories. Every object
    in OFS is a file. These files make up the nodes of a graph. They can be
    linked together with bidirectional edges.
    </p>

    <p>
    This FS wasn't designed to be completely practical, but rather for novelty.
    I don't know of any such graph-based file system in the wild. There is a good
    reason for this: graphs are pointer-based data structures by nature. Pointers
    are the bane of file systems. Researchers make extravagent efforts to avoid
    random disk accesses because they are super slow on hard disk drives. That said,
    supposedly the random access penalty is not too bad on SSDs, so maybe this might
    be practical in the future. Anyhow...
    </p>

    <p>
    One file (inode #0) is designated as the "root" file. This is a bit of a
    misnomer because the graph is not necessarily a tree; it can have cycles.
    However, it does serve as a fixed point in the file system from which all
    other files can be reached. A path is simply a slash-separated list of
    filenames from the root to the destination.
    </p>

    <p>
    For now, the file system is read-only. I still need to think about how to handle
    consistency issues. However, for the purposes of documentation, I will describe
    my end goals here.
    </p>

    <h2>API</h2>

    <p>
    OFS has a pretty straight-forward API. I might augment it for functionality later,
    but the basics should be the same:
    </p>

    <table id="ofs-table">
        <thead>
            <tr>
                <th>Operation</th>
                <th>Functionality</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>stat</td>
                <td>Returns a copy of the inode of the specified file. As described
                below, from an inode, you can tell the metadata and links of a file.</td>
            </tr>
            <tr>
                <td>new_file</td>
                <td>Create a new file.</td>
            </tr>
            <tr>
                <td>delete_file</td>
                <td>Delete the file by removing all of its links and free its inode and
                dnodes.</td>
            </tr>
            <tr>
                <td>open</td>
                <td>Open the file, returning a file handle.</td>
            </tr>
            <tr>
                <td>link</td>
                <td>Create a link between two files.</td>
            </tr>
            <tr>
                <td>unlink</td>
                <td>Remove the link between two files.</td>
            </tr>
            <tr>
                <td>change_meta</td>
                <td>Change the metadata of a file.</td>
            </tr>
            <tr>
                <td>seek</td>
                <td>Seek to the given location in the file.</td>
            </tr>
            <tr>
                <td>read</td>
                <td>Read from the file.</td>
            </tr>
            <tr>
                <td>write</td>
                <td>Write to the file.</td>
            </tr>
        </tbody>
    </table>

    <h2>Disk-level Representation</h2>

    <p>
    For simplicity, os1 treats the whole of hdd as a single partition. The
    first sector contains file system metadata. For now, this consists of 4
    magic bytes, the inode count, and the dnode count.  The next sectors
    contain the inode and dnode bitmaps, in that order. The rest of the drive
    contains the inodes and dnodes themselves.
    </p>

    <p>
    Each inode represents a single file and contains the file metadata. This
    includes the filename, permissions, dates modified and created, links, and
    the dnode-number of the first dnode. The list of links can conclude with
    the index of a dnode containing extra links.
    </p>

    <p>
    Each dnode contains the contents of the file. The last word of the dnode is
    the index of the next dnode.
    </p>

    <h2>OFS Troll</h2>

    <p>
    Deleting a file may disconnect the graph. If this happens, the portion
    disconnected from the "root" needs to be deleted. This is the job of the OFS
    Troll. The Troll is a process that does a breadth-first search from the root
    searching for disconnected components.
    </p>

    <p>
    Since the Troll is already going through all the inodes, it seems advantageous
    to also have it function as a basic live fsck. I haven't really thought through
    this too much yet.
    </p>

    <p>
    (You might also be wondering why I called it a "Troll". Well, operating systems
    already have "zombies" and "reapers", so why not trolls?)
    </p>

</div>
</body>
</html>
